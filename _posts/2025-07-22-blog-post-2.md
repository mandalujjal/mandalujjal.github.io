---
title: 'CUDA Architecture Explained Simply!'
date: 2025-07-22
permalink: /posts/2025/07/blog-post-2/
tags:
  - cool posts
  - category1
  - category2
---


# CUDA Architecture Explained Simply for Beginners

CUDA (Compute Unified Device Architecture) হল NVIDIA-এর তৈরি একটি প্যারালেল কম্পিউটিং প্ল্যাটফর্ম যা GPU-তে জেনারেল-পারপাস কম্পিউটিং (GPGPU) করার সুবিধা দেয়।

## সহজভাবে CUDA আর্কিটেকচার বোঝার উপায়

**১. CPU vs GPU:**
- CPU: কয়েকটি কোর (সাধারণত ৪-৩২), প্রতিটি কোর খুব দ্রুত, জটিল টাস্কের জন্য উপযুক্ত
- GPU: হাজার হাজার ছোট কোর, সহজ কিন্তু প্যারালেল কাজের জন্য অপ্টিমাইজড

**২. CUDA-র মূল ধারণা:**
- একই কাজ অনেক ডাটার উপর একসাথে করা (SIMD - Single Instruction Multiple Data)
- উদাহরণ: ১০০০টি সংখ্যার each সংখ্যাকে ২ দিয়ে গুণ করা

**৩. CUDA আর্কিটেকচারের প্রধান অংশ:**

**ক) হোস্ট (CPU) ও ডিভাইস (GPU):**
- প্রোগ্রাম CPU-তে চলে, কিন্তু ইনটেনসিভ গণনা GPU-তে পাঠায়

**খ) থ্রেড হায়ারার্কি:**
- থ্রেড: সবচেয়ে ছোট একক (একটি কাজ করে)
- ব্লক: একসাথে কাজ করা থ্রেডের গ্রুপ (সাধারণত ৩২-১০২৪ থ্রেড)
- গ্রিড: একসাথে কাজ করা ব্লকের গ্রুপ

**গ) GPU মেমোরি হায়ারার্কি:**
- গ্লোবাল মেমোরি: সব থ্রেড এক্সেস করতে পারে (ধীর)
- শেয়ার্ড মেমোরি: একটি ব্লকের থ্রেডরা শেয়ার করে (দ্রুত)
- রেজিস্টার: প্রতিটি থ্রেডের নিজস্ব (সবচেয়ে দ্রুত)

## কিভাবে CUDA প্রোগ্রাম কাজ করে:
১. CPU মেমোরি থেকে GPU মেমোরিতে ডাটা কপি হয়
২. GPU-তে অনেক থ্রেড একসাথে কাজ করে
৩. ফলাফল GPU থেকে CPU মেমোরিতে ফেরত আসে

## সহজ উদাহরণ:
ধরা যাক আপনি ১০০০টি ছাত্রের মার্কসকে ২ দিয়ে গুণ করতে চান:
- CPU-তে: ১০০০ বার লুপ চালাতে হবে (ধারাবাহিকভাবে)
- CUDA-তে: ১০০০ থ্রেড একসাথে কাজ করবে (প্রতিটি থ্রেড একটি মার্কস প্রসেস করবে)




![cuda1](https://github.com/user-attachments/assets/6d0e9af7-b5a2-485a-9347-4d32b8fb12fa)




### **CUDA Execution Flowchart**   

#### **Flowchart Steps** (Text + Visual):  
1. **CPU (Host)**  
   - Allocates memory on GPU  
   - Copies input data (e.g., array) from CPU → GPU  

2. **GPU (Device)**  
   - **Kernel Launch**: CPU starts parallel task on GPU  
     - Grid of blocks → Blocks of threads  
   - **Parallel Execution**: All threads run the same kernel code on different data  
     *(e.g., Thread #1 processes Data[0], Thread #2 processes Data[1], etc.)*  

3. **CPU (Host)**  
   - Copies results back from GPU → CPU  
   - Frees GPU memory  

#### **Visual Flowchart**:  
```
[CPU] 
  ↓ (Allocate GPU memory + Copy input data)  
[GPU] 
  ↓ (Kernel Launch → Grid → Blocks → Threads)  
  ↓ (Parallel Execution)  
[CPU] 
  ↓ (Copy results back + Free memory)
```

---


