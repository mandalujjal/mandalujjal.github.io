---
title: 'CUDA Architecture Explained Simply!'
date: 2025-07-22
permalink: /posts/2025/07/blog-post-2/
tags:
  - cool posts
  - category1
  - category2
---


# CUDA Architecture Explained Simply for Beginners

CUDA (Compute Unified Device Architecture) হল NVIDIA-এর তৈরি একটি প্যারালেল কম্পিউটিং প্ল্যাটফর্ম যা GPU-তে জেনারেল-পারপাস কম্পিউটিং (GPGPU) করার সুবিধা দেয়।

## সহজভাবে CUDA আর্কিটেকচার বোঝার উপায়

**১. CPU vs GPU:**
- CPU: কয়েকটি কোর (সাধারণত ৪-৩২), প্রতিটি কোর খুব দ্রুত, জটিল টাস্কের জন্য উপযুক্ত
- GPU: হাজার হাজার ছোট কোর, সহজ কিন্তু প্যারালেল কাজের জন্য অপ্টিমাইজড

**২. CUDA-র মূল ধারণা:**
- একই কাজ অনেক ডাটার উপর একসাথে করা (SIMD - Single Instruction Multiple Data)
- উদাহরণ: ১০০০টি সংখ্যার each সংখ্যাকে ২ দিয়ে গুণ করা

**৩. CUDA আর্কিটেকচারের প্রধান অংশ:**

**ক) হোস্ট (CPU) ও ডিভাইস (GPU):**
- প্রোগ্রাম CPU-তে চলে, কিন্তু ইনটেনসিভ গণনা GPU-তে পাঠায়

**খ) থ্রেড হায়ারার্কি:**
- থ্রেড: সবচেয়ে ছোট একক (একটি কাজ করে)
- ব্লক: একসাথে কাজ করা থ্রেডের গ্রুপ (সাধারণত ৩২-১০২৪ থ্রেড)
- গ্রিড: একসাথে কাজ করা ব্লকের গ্রুপ

**গ) GPU মেমোরি হায়ারার্কি:**
- গ্লোবাল মেমোরি: সব থ্রেড এক্সেস করতে পারে (ধীর)
- শেয়ার্ড মেমোরি: একটি ব্লকের থ্রেডরা শেয়ার করে (দ্রুত)
- রেজিস্টার: প্রতিটি থ্রেডের নিজস্ব (সবচেয়ে দ্রুত)

## কিভাবে CUDA প্রোগ্রাম কাজ করে:
১. CPU মেমোরি থেকে GPU মেমোরিতে ডাটা কপি হয়
২. GPU-তে অনেক থ্রেড একসাথে কাজ করে
৩. ফলাফল GPU থেকে CPU মেমোরিতে ফেরত আসে

## সহজ উদাহরণ:
ধরা যাক আপনি ১০০০টি ছাত্রের মার্কসকে ২ দিয়ে গুণ করতে চান:
- CPU-তে: ১০০০ বার লুপ চালাতে হবে (ধারাবাহিকভাবে)
- CUDA-তে: ১০০০ থ্রেড একসাথে কাজ করবে (প্রতিটি থ্রেড একটি মার্কস প্রসেস করবে)


### **CUDA স্ট্যাকের লেয়ারগুলি কীভাবে একে অপরের উপর নির্ভরশীল**  

CUDA সফটওয়্যার স্ট্যাক নিচের মতো স্তরবদ্ধ (layer-by-layer)ভাবে কাজ করে:  

```
[CUDA Libraries (e.g., cuBLAS, cuFFT)]  
       ↓  
[CUDA Runtime API]  
       ↓  
[CUDA Driver API]  
       ↓  
[GPU Hardware (NVIDIA GPU)]
```

---

## **১. কে কার উপর নির্ভর করে?**  
### **(১) CUDA Libraries (cuBLAS, cuFFT, etc.)**  
- **নির্ভরশীল:** CUDA Runtime API বা সরাসরি Driver API-র উপর।  
- **কাজ:** হাই-লেভেল অপারেশন (ম্যাট্রিক্স মাল্টিপ্লিকেশন, FFT ইত্যাদি) অটোমেটিকভাবে GPU-তে চালায়।  
- **উদাহরণ:**  
  ```cpp
  cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k, &alpha, A, lda, B, ldb, &beta, C, ldc);
  ```
  ↳ `cuBLAS` লাইব্রেরি ভিতরে CUDA Runtime/Driver API কল করে GPU-তে কাজ চালায়।  

---

### **(২) CUDA Runtime API**  
- **নির্ভরশীল:** CUDA Driver API-র উপর।  
- **কাজ:**  
  - GPU মেমোরি ম্যানেজমেন্ট (`cudaMalloc`, `cudaMemcpy`)।  
  - কার্নেল লঞ্চ (`kernel<<<blocks, threads>>>()` সিনট্যাক্স)।  
- **উদাহরণ:**  
  ```cpp
  cudaMalloc(&d_array, size);  // Runtime API → ভিতরে Driver API-র `cuMemAlloc` কল করে।
  ```

---

### **(৩) CUDA Driver API**  
- **নির্ভরশীল:** সরাসরি NVIDIA GPU ড্রাইভার (`nvidia.ko` / `nvidia.sys`) এবং হার্ডওয়্যারের উপর।  
- **কাজ:**  
  - GPU ডিভাইস ইনিশিয়ালাইজ (`cuInit`), কনটেক্সট ম্যানেজমেন্ট (`cuCtxCreate`)।  
  - লো-লেভেল কার্নেল এক্সিকিউশন (`cuLaunchKernel`)।  
- **উদাহরণ:**  
  ```cpp
  cuMemAlloc(&d_array, size);  // সরাসরি GPU ড্রাইভারকে মেমোরি অ্যালোকেশনের জন্য বলে।
  ```

---

### **(৪) GPU Hardware**  
- **নির্ভরশীল:** NVIDIA-এর ফিজিক্যাল আর্কিটেকচার (CUDA Cores, Tensor Cores, Memory Hierarchy)।  
- **কাজ:** সমস্ত প্যারালেল কম্পিউটিং অপারেশন এক্সিকিউট করে।  

---

## **২. ডিপেন্ডেন্সি সারমর্ম**  
| **স্তর**          | **এর উপর নির্ভর করে**          | **যে ফাংশনগুলি ব্যবহার করে**          |  
|-------------------|-------------------------------|--------------------------------------|  
| **CUDA Libraries**| Runtime API / Driver API      | `cublasCreate()`, `cufftPlan1d()`    |  
| **Runtime API**   | Driver API                    | `cudaMalloc()`, `cudaMemcpy()`       |  
| **Driver API**    | GPU Driver + Hardware         | `cuMemAlloc()`, `cuLaunchKernel()`   |  
| **GPU**          | NVIDIA ফিজিক্যাল আর্কিটেকচার   | SM (Streaming Multiprocessors), GDDR6 |  

---

## **৩. ডায়াগ্রাম: কিভাবে ডাটা ফ্লো হয়**  
```
[Your CUDA Program]  
    │  
    ├─> [CUDA Libraries] → Runtime API → Driver API → GPU  
    └─> [Direct Runtime/Driver Calls] → GPU  
```

---

## **৪. কোন লেয়ার কখন ব্যবহার করবেন?**  
- **এপ্লিকেশন ডেভেলপার (আপনি):**  
  - CUDA Libraries (cuBLAS/cuFFT) → দ্রুত ডেভেলপমেন্টের জন্য।  
  - CUDA Runtime API → কাস্টম কার্নেল লিখতে হলে।  
- **লাইব্রেরি/ফ্রেমওয়ার্ক ডেভেলপার (TensorFlow, PyTorch):**  
  - Driver API → ম্যাক্সিমাম কন্ট্রোলের জন্য।  

---





![cuda1](https://github.com/user-attachments/assets/6d0e9af7-b5a2-485a-9347-4d32b8fb12fa)




### **CUDA Execution Flowchart**   

#### **Flowchart Steps** (Text + Visual):  
1. **CPU (Host)**  
   - Allocates memory on GPU  
   - Copies input data (e.g., array) from CPU → GPU  

2. **GPU (Device)**  
   - **Kernel Launch**: CPU starts parallel task on GPU  
     - Grid of blocks → Blocks of threads  
   - **Parallel Execution**: All threads run the same kernel code on different data  
     *(e.g., Thread #1 processes Data[0], Thread #2 processes Data[1], etc.)*  

3. **CPU (Host)**  
   - Copies results back from GPU → CPU  
   - Frees GPU memory  

#### **Visual Flowchart**:  
```
[CPU] 
  ↓ (Allocate GPU memory + Copy input data)  
[GPU] 
  ↓ (Kernel Launch → Grid → Blocks → Threads)  
  ↓ (Parallel Execution)  
[CPU] 
  ↓ (Copy results back + Free memory)
```

---


![cuda2](https://github.com/user-attachments/assets/d725c5fa-eff6-459a-94e2-747a2f7dbe43)
